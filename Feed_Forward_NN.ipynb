{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed Forward NN",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilynairnn/msci598_final_project/blob/main/Feed_Forward_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "de77LjQ8TT3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras_self_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63gEbOedticN",
        "outputId": "b69b3ed5-045f-475e-e4f9-be7fe64266d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_self_attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.21.5)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=0e57a1c0931608b805db62525b402f1c7c6285df96f31e2a6e290fcb8bb2c805\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb5KjpyxFpAK",
        "outputId": "b05a1029-c968-4810-c2cf-2854070b01d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'gdrive/My Drive/MSCI598 Project')\n",
        "from utils.score import report_score, LABELS, score_submission\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import Sequential, regularizers\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import initializers, constraints, regularizers\n",
        "from keras.layers import Reshape, Dot, Concatenate, Input, Embedding, Dropout, Dense, LSTM, Bidirectional, Activation, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csr_matrix\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import Dense, Activation, Flatten, SimpleRNN, GRU\n",
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'gdrive/My Drive/MSCI598 Project/GoogleNews-vectors-negative300.bin'\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(dir, binary=True)"
      ],
      "metadata": {
        "id": "GN2Artmiof9m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Processing Data"
      ],
      "metadata": {
        "id": "XDOcMacHTZZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set parameters\n",
        "max_feat = 5000\n",
        "batch = 128\n",
        "num_epochs = 10\n",
        "max_vocab = 30000\n",
        "max_sent_len = 30\n",
        "embedding_dim = 300\n",
        "lstm_dim = 128"
      ],
      "metadata": {
        "id": "b6GTfTmyVYcK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will tokenize the take, make all words lower case and extract any stopwords \n",
        "def tokenize(content):\n",
        "  list = \" \".join(re.findall(r'\\w+', content, flags=re.UNICODE)).lower()\n",
        "  return \" \".join([word for word in content.split(\" \") if word not in feature_extraction.text.ENGLISH_STOP_WORDS])\n",
        "\n",
        "# the raw data in csv files are stored in the google drive \n",
        "\n",
        "# the training data it processed by reading in the files and running the tokenize method above to clean the data \n",
        "train_bodies = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/train_bodies.csv')\n",
        "train_stances = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/train_stances.csv')\n",
        "train_combined = train_stances.join(train_bodies.set_index('Body ID'), on='Body ID')\n",
        "train_headlines_clean = [tokenize(headline) for headline in train_combined['Headline']]\n",
        "train_bodies_clean = [tokenize(article_body) for article_body in train_combined['articleBody']]\n",
        "\n",
        "# the test data it processed by reading in the files and running the tokenize method above to clean the data \n",
        "test_bodies = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/competition_test_bodies.csv')\n",
        "test_stances = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/competition_test_stances.csv')\n",
        "test_combined = test_stances.join(test_bodies.set_index('Body ID'), on='Body ID')\n",
        "test_headlines_clean = [tokenize(headline) for headline in test_combined['Headline']]\n",
        "test_bodies_clean = [tokenize(article_body) for article_body in test_combined['articleBody']]"
      ],
      "metadata": {
        "id": "PZ2ThD5kzpMn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the keras preprocessing package text_to_word_sequence is ran on the test and training headlines and bodies\n",
        "# this splits up the text into a list of words so that it can be iterated on \n",
        "\n",
        "train_headlines_words = [text_to_word_sequence(headline) for headline in train_headlines_clean]\n",
        "train_bodies_words = [text_to_word_sequence(article_body) for article_body in train_bodies_clean]\n",
        "test_headlines_words = [text_to_word_sequence(headline) for headline in test_headlines_clean]\n",
        "test_bodies_words = [text_to_word_sequence(article_body) for article_body in test_bodies_clean]\n",
        "\n",
        "# words from headlines and bodies are iterated on and added to an array\n",
        "train_words_all = [None]*len(train_headlines_words)\n",
        "for i in range(len(train_headlines_words)):\n",
        "  train_words_all[i] = train_headlines_words[i] + train_bodies_words[i]\n",
        "\n",
        "test_words_all = [None]*len(test_headlines_words)\n",
        "for i in range(len(test_headlines_words)):\n",
        "  test_words_all[i] = test_headlines_words[i] + test_bodies_words[i]\n"
      ],
      "metadata": {
        "id": "L4pG-eFkJag7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = train_words_all + test_words_all\n",
        "# use tokenizer from Keras preprocessing to create train and test data sets using label encoder \n",
        "# have 4 classes for the 4 different ways to classify article \n",
        "tokenizer = Tokenizer(num_words = max_vocab)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:max_sent_len]) for seq in all_words])\n",
        "\n",
        "trainX = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_len]) for seq in train_words_all])\n",
        "trainX = pad_sequences(trainX, maxlen = max_sent_len, padding = 'post',truncating = 'post')\n",
        "label_encoder_train = LabelEncoder().fit_transform(train_combined['Stance'])\n",
        "trainY = np_utils.to_categorical(label_encoder_train, num_classes = 4)\n",
        "\n",
        "testX = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_len]) for seq in test_words_all])\n",
        "testX = pad_sequences(testX, maxlen = max_sent_len, padding = 'post',truncating = 'post')\n",
        "label_encoder_test = LabelEncoder().fit_transform(test_combined['Stance'])\n",
        "testY = np_utils.to_categorical(label_encoder_test, num_classes = 4)\n",
        "\n",
        "# split train into train and validation datasets \n",
        "trainX, valX, trainY, valY = train_test_split(trainX, trainY, random_state=10, test_size=0.1)\n"
      ],
      "metadata": {
        "id": "DG1np5rFMMTQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "model = Sequential() \n",
        "model.add(Input(shape=(4,), name='Input')) \n",
        "model.add(Dense(32, activation='softmax', name='Hidden'))\n",
        "model.add(Dense(4, activation='sigmoid', name='Output')) \n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yftQBJYaB-Wc",
        "outputId": "5db7aa42-f830-446e-aa35-fc5929022203"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Hidden (Dense)              (None, 32)                160       \n",
            "                                                                 \n",
            " Output (Dense)              (None, 4)                 132       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 292\n",
            "Trainable params: 292\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model using training data and validation data\n",
        "model.fit(trainX,trainY,\n",
        "          batch_size = batch,\n",
        "          epochs = num_epochs,\n",
        "          validation_data=(valX, valY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwVsEr-V_Bmx",
        "outputId": "2b547eb2-0021-45bd-a945-da406df9c66c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 1.4231 - accuracy: 0.0737 - val_loss: 1.3882 - val_accuracy: 0.0766\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 1.3558 - accuracy: 0.0865 - val_loss: 1.3206 - val_accuracy: 0.1357\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 1.2898 - accuracy: 0.3059 - val_loss: 1.2549 - val_accuracy: 0.6959\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 1.2268 - accuracy: 0.7232 - val_loss: 1.1995 - val_accuracy: 0.7287\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 1.1556 - accuracy: 0.7314 - val_loss: 1.1032 - val_accuracy: 0.7291\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 1.0601 - accuracy: 0.7316 - val_loss: 1.0221 - val_accuracy: 0.7291\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 0.9887 - accuracy: 0.7316 - val_loss: 0.9639 - val_accuracy: 0.7291\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 0.9372 - accuracy: 0.7316 - val_loss: 0.9195 - val_accuracy: 0.7291\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 0.8988 - accuracy: 0.7316 - val_loss: 0.8925 - val_accuracy: 0.7291\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 0.8779 - accuracy: 0.7316 - val_loss: 0.8742 - val_accuracy: 0.7291\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f187c0e4a50>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(testX, testY,\n",
        "               batch_size = batch)\n",
        "\n",
        "predicted = [LABELS[np.argmax(i)] for i in model.predict(testX)]\n",
        "actual = [LABELS[np.argmax(i)] for i in testY]\n",
        "np.savetxt(\"answer.csv\", predicted, delimiter=\",\", fmt='%s')\n",
        "report_score(actual,predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykluSEE4pHK3",
        "outputId": "04df63f6-dd81-44aa-8f29-3ab995e49784"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199/199 [==============================] - 0s 2ms/step - loss: 0.8896 - accuracy: 0.7220\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |     0     |     0     |     0     |   1903    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |     0     |     0     |     0     |    697    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |     0     |     0     |     0     |   4464    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |     0     |     0     |     0     |   18349   |\n",
            "-------------------------------------------------------------\n",
            "Score: 4587.25 out of 11651.25\t(39.37131209097736%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39.37131209097736"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df.to_csv('output.csv', index=False, encoding = 'utf-8') \n",
        "files.download('answer.csv')"
      ],
      "metadata": {
        "id": "oiVtZ4r-ZdfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}