{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilynairnn/msci598_final_project/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "muH_UCTCSaiH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb5KjpyxFpAK",
        "outputId": "e42f2d1a-6300-4013-daed-78af9cb8aaa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'gdrive/My Drive/MSCI598 Project')\n",
        "from utils.score import report_score, LABELS, score_submission\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import Sequential, regularizers\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import initializers, constraints, regularizers\n",
        "from keras.layers import Reshape, Dot, Concatenate, Input, Embedding, Dropout, Dense, LSTM, Bidirectional, Activation, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build word2vec model \n",
        "dir = 'gdrive/My Drive/MSCI598 Project/GoogleNews-vectors-negative300.bin'\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(dir, binary=True)"
      ],
      "metadata": {
        "id": "GN2Artmiof9m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Processing Data"
      ],
      "metadata": {
        "id": "Uib8eChpSflj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set parameters\n",
        "max_feat = 5000\n",
        "batch = 128\n",
        "num_epochs = 10\n",
        "max_vocab = 30000\n",
        "max_sent_len = 30\n",
        "embedding_dim = 300\n",
        "lstm_dim = 128"
      ],
      "metadata": {
        "id": "b6GTfTmyVYcK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will tokenize the take, make all words lower case and extract any stopwords \n",
        "def tokenize(content):\n",
        "  list = \" \".join(re.findall(r'\\w+', content, flags=re.UNICODE)).lower()\n",
        "  return \" \".join([word for word in content.split(\" \") if word not in feature_extraction.text.ENGLISH_STOP_WORDS])\n",
        "\n",
        "# the raw data in csv files are stored in the google drive \n",
        "\n",
        "# the training data it processed by reading in the files and running the tokenize method above to clean the data \n",
        "train_bodies = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/train_bodies.csv')\n",
        "train_stances = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/train_stances.csv')\n",
        "train_combined = train_stances.join(train_bodies.set_index('Body ID'), on='Body ID')\n",
        "train_headlines_clean = [tokenize(headline) for headline in train_combined['Headline']]\n",
        "train_bodies_clean = [tokenize(article_body) for article_body in train_combined['articleBody']]\n",
        "\n",
        "# the test data it processed by reading in the files and running the tokenize method above to clean the data \n",
        "test_bodies = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/competition_test_bodies.csv')\n",
        "test_stances = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/competition_test_stances.csv')\n",
        "test_combined = test_stances.join(test_bodies.set_index('Body ID'), on='Body ID')\n",
        "test_headlines_clean = [tokenize(headline) for headline in test_combined['Headline']]\n",
        "test_bodies_clean = [tokenize(article_body) for article_body in test_combined['articleBody']]\n"
      ],
      "metadata": {
        "id": "PZ2ThD5kzpMn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the keras preprocessing package text_to_word_sequence is ran on the test and training headlines and bodies\n",
        "# this splits up the text into a list of words so that it can be iterated on \n",
        "\n",
        "train_headlines_words = [text_to_word_sequence(headline) for headline in train_headlines_clean]\n",
        "train_bodies_words = [text_to_word_sequence(article_body) for article_body in train_bodies_clean]\n",
        "test_headlines_words = [text_to_word_sequence(headline) for headline in test_headlines_clean]\n",
        "test_bodies_words = [text_to_word_sequence(article_body) for article_body in test_bodies_clean]\n",
        "\n",
        "# words from headlines and bodies are iterated on and added to an array\n",
        "train_words_all = [None]*len(train_headlines_words)\n",
        "for i in range(len(train_headlines_words)):\n",
        "  train_words_all[i] = train_headlines_words[i] + train_bodies_words[i]\n",
        "\n",
        "test_words_all = [None]*len(test_headlines_words)\n",
        "for i in range(len(test_headlines_words)):\n",
        "  test_words_all[i] = test_headlines_words[i] + test_bodies_words[i]\n"
      ],
      "metadata": {
        "id": "L4pG-eFkJag7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the amount of words in different percentiles \n",
        "all_words = train_words_all + test_words_all\n",
        "lengths = []\n",
        "totlength = 0\n",
        "for list in all_words[:10]:\n",
        "  length = len(list)\n",
        "  lengths.append(length)\n",
        "  totlength = totlength + length\n",
        "\n",
        "avglength = np.percentile (lengths, 50)\n",
        "ninetieth = np.percentile(lengths, 90)\n",
        "tenth = np.percentile(lengths, 10)\n",
        "print(\"average length: \" + str(avglength))\n",
        "print(\"ninetieth percentile: \" + str(ninetieth))\n",
        "print(\"tenth percentile: \" + str(tenth))\n"
      ],
      "metadata": {
        "id": "DG1np5rFMMTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d15cb8-3257-4d58-a1b6-8e9533d03e93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average length: 217.5\n",
            "ninetieth percentile: 352.5999999999999\n",
            "tenth percentile: 111.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use tokenizer from Keras preprocessing to create train and test data sets using label encoder \n",
        "# have 4 classes for the 4 different ways to classify article \n",
        "tokenizer = Tokenizer(num_words = max_vocab)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:max_sent_len]) for seq in all_words])\n",
        "\n",
        "trainX = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_len]) for seq in train_words_all])\n",
        "trainX = pad_sequences(trainX, maxlen = max_sent_len, padding = 'post',truncating = 'post')\n",
        "label_encoder_train = LabelEncoder().fit_transform(train_combined['Stance'])\n",
        "trainY = np_utils.to_categorical(label_encoder_train, num_classes = 4)\n",
        "\n",
        "testX = tokenizer.texts_to_sequences([' '.join(seq[:max_sent_len]) for seq in test_words_all])\n",
        "testX = pad_sequences(testX, maxlen = max_sent_len, padding = 'post',truncating = 'post')\n",
        "label_encoder_test = LabelEncoder().fit_transform(test_combined['Stance'])\n",
        "testY = np_utils.to_categorical(label_encoder_test, num_classes = 4)\n",
        "\n",
        "# split train into train and validation datasets \n",
        "trainX, valX, trainY, valY = train_test_split(trainX, trainY, random_state=10, test_size=0.1)\n"
      ],
      "metadata": {
        "id": "_PqJrCBC1MJ_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Model"
      ],
      "metadata": {
        "id": "x2DhH7DASlR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build model \n",
        "\n",
        "# create embeddings matrix\n",
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  try:\n",
        "    embeddings_vector = wv[word]\n",
        "  except KeyError:\n",
        "    embeddings_vector = None\n",
        "  if embeddings_vector is not None:\n",
        "    embeddings_matrix[i] = embeddings_vector\n",
        "\n",
        "# build LSTM RNN \n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embeddings_matrix], trainable=False, name='word_embedding_layer',\n",
        "                    mask_zero=True))\n",
        "model.add(LSTM(lstm_dim, return_sequences=False, name='lstm_layer'))\n",
        "model.add(Dense(64, name='dense_1'))\n",
        "model.add(BatchNormalization(name='bn_1'))\n",
        "model.add(Dropout(rate=0.25, name='dropout_1'))\n",
        "model.add(Activation(activation='relu', name='activation_1'))   #change hidden layer here (sigmoid, tanh, relu)\n",
        "model.add(Dense(4, activation='softmax', name='output_layer'))  #was sigmoid\n",
        "model.summary()\n",
        "\n",
        "# optimizer to change learning rate\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# compile the model and calculate accuracy\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit the model using training data and validation data\n",
        "model.fit(trainX,trainY,\n",
        "          batch_size = batch,\n",
        "          epochs = num_epochs,\n",
        "          validation_data=(valX, valY))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsN_8qHHOYzp",
        "outputId": "7795ea72-124b-4d1a-e2ca-121a84e30991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " word_embedding_layer (Embed  (None, None, 300)        3470400   \n",
            " ding)                                                           \n",
            "                                                                 \n",
            " lstm_layer (LSTM)           (None, 128)               219648    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " bn_1 (BatchNormalization)   (None, 64)                256       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,698,820\n",
            "Trainable params: 228,292\n",
            "Non-trainable params: 3,470,528\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "352/352 [==============================] - 58s 152ms/step - loss: 0.7640 - accuracy: 0.7159 - val_loss: 0.5509 - val_accuracy: 0.7917\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 53s 150ms/step - loss: 0.4797 - accuracy: 0.8226 - val_loss: 0.4573 - val_accuracy: 0.8277\n",
            "Epoch 3/10\n",
            "255/352 [====================>.........] - ETA: 14s - loss: 0.3736 - accuracy: 0.8601"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "0X8HFsRLSnTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on test data\n",
        "model.evaluate(testX, testY,\n",
        "               batch_size = batch)\n",
        "\n",
        "predicted = [LABELS[np.argmax(i)] for i in model.predict(testX)]\n",
        "actual = [LABELS[np.argmax(i)] for i in testY]\n",
        "np.savetxt(\"predicted.csv\", predicted, delimiter=\",\", fmt='%s')\n",
        "np.savetxt(\"actual.csv\", actual, delimiter=\",\", fmt='%s')\n",
        "!cp predicted.csv \"gdrive/My Drive/MSCI598 Project\"\n",
        "!cp actual.csv \"gdrive/My Drive/MSCI598 Project\"\n",
        "report_score(actual,predicted)"
      ],
      "metadata": {
        "id": "mS7J3ikJZjTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb1366d-4c27-410c-db7e-5aca3b9a0325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199/199 [==============================] - 13s 65ms/step - loss: 1.1345 - accuracy: 0.7113\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |    592    |     9     |    207    |   1095    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |    142    |     7     |    72     |    476    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |    647    |    16     |   1492    |   2309    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |   1227    |    51     |   1086    |   15985   |\n",
            "-------------------------------------------------------------\n",
            "Score: 6360.5 out of 11651.25\t(54.590709151378604%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54.590709151378604"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save"
      ],
      "metadata": {
        "id": "UJu2cLGwAoDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df.to_csv('output.csv', index=False, encoding = 'utf-8') \n",
        "files.download('answer.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FKMvYfF0i4Mx",
        "outputId": "659c3648-cb84-4dd0-e495-d591b3e744c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a0e2a578-7480-41fb-ae55-f6c1ec2bf6f7\", \"answer.csv\", 2175891)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}