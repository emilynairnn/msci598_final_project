{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM with attention ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilynairnn/msci598_final_project/blob/main/LSTM_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n"
      ],
      "metadata": {
        "id": "JMelH5RCpfZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras_self_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63gEbOedticN",
        "outputId": "adac6c6d-4d89-47ab-fa2d-527cc51e7778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_self_attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.21.5)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=825abd7b01ed4da8174b36d6a994ac5df28119a27afa2601ee8ca649b95fe25a\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb5KjpyxFpAK",
        "outputId": "475777dc-21e5-4144-a5f3-e7f90dbfb95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'gdrive/My Drive/MSCI598 Project')\n",
        "from utils.score import report_score, LABELS, score_submission\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import feature_extraction\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import Sequential, regularizers\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import initializers, constraints, regularizers\n",
        "from keras.layers import Reshape, Dot, Concatenate, Input, Embedding, Dropout, Dense, LSTM, Bidirectional, Activation, BatchNormalization, Flatten\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csr_matrix\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras import optimizers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'gdrive/My Drive/MSCI598 Project/GoogleNews-vectors-negative300.bin'\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(dir, binary=True)"
      ],
      "metadata": {
        "id": "GN2Artmiof9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_FEATURES = 5000\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 10\n",
        "MAX_VOCAB_SIZE = 30000\n",
        "MAX_SENT_LEN = 2\n",
        "EMBEDDING_DIM = 300\n",
        "LSTM_DIM = 128"
      ],
      "metadata": {
        "id": "b6GTfTmyVYcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lstm"
      ],
      "metadata": {
        "id": "ecqqVRP6Hk5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing code "
      ],
      "metadata": {
        "id": "VXStpHh3Bg7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(content):\n",
        "  list = \" \".join(re.findall(r'\\w+', content, flags=re.UNICODE)).lower()\n",
        "  return \" \".join([word for word in content.split(\" \") if word not in feature_extraction.text.ENGLISH_STOP_WORDS])\n",
        "\n",
        "train_bodies = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/train_bodies.csv')\n",
        "train_stances = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/train_stances.csv')\n",
        "train_combined = train_stances.join(train_bodies.set_index('Body ID'), on='Body ID')\n",
        "\n",
        "test_bodies = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/competition_test_bodies.csv')\n",
        "test_stances = pd.read_csv('gdrive/My Drive/MSCI598 Project/fnc-1-master/competition_test_stances.csv')\n",
        "test_combined = test_stances.join(test_bodies.set_index('Body ID'), on='Body ID')\n",
        "\n",
        "train_headlines_clean = [clean(headline) for headline in train_combined['Headline']]\n",
        "train_bodies_clean = [clean(body) for body in train_combined['articleBody']]\n",
        "\n",
        "test_headlines_clean = [clean(headline) for headline in test_combined['Headline']]\n",
        "test_bodies_clean = [clean(body) for body in test_combined['articleBody']]\n"
      ],
      "metadata": {
        "id": "PZ2ThD5kzpMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_word_seq_headlines = [text_to_word_sequence(headline) for headline in train_headlines_clean]\n",
        "train_word_seq_bodies = [text_to_word_sequence(body) for body in train_bodies_clean]\n",
        "test_word_seq_headlines = [text_to_word_sequence(headline) for headline in test_headlines_clean]\n",
        "test_word_seq_bodies = [text_to_word_sequence(body) for body in test_bodies_clean]\n",
        "\n",
        "train_word_seq = [None]*len(train_word_seq_headlines)\n",
        "for i in range(len(train_word_seq_headlines)):\n",
        "  train_word_seq[i] = train_word_seq_headlines[i] + train_word_seq_bodies[i]\n",
        "\n",
        "test_word_seq = [None]*len(test_word_seq_headlines)\n",
        "for i in range(len(test_word_seq_headlines)):\n",
        "  test_word_seq[i] = test_word_seq_headlines[i] + test_word_seq_bodies[i]\n"
      ],
      "metadata": {
        "id": "L4pG-eFkJag7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "word_seq = train_word_seq + test_word_seq\n",
        "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in train_word_seq])\n",
        "X_train = pad_sequences(X_train, maxlen = MAX_SENT_LEN, padding = 'post',truncating = 'post')\n",
        "label_encoder_train = LabelEncoder().fit_transform(train_combined['Stance'])\n",
        "Y_train = np_utils.to_categorical(label_encoder_train, num_classes = 4)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in test_word_seq])\n",
        "X_test = pad_sequences(X_test, maxlen = MAX_SENT_LEN, padding = 'post',truncating = 'post')\n",
        "label_encoder_test = LabelEncoder().fit_transform(test_combined['Stance'])\n",
        "Y_test = np_utils.to_categorical(label_encoder_test, num_classes = 4)\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, random_state=10, test_size=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "DG1np5rFMMTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d2c08f-6696-4620-aa92-6a713d920cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[140]\n",
            " [ 14]\n",
            " [174]\n",
            " [ 98]\n",
            " [520]\n",
            " [279]\n",
            " [465]\n",
            " [212]\n",
            " [ 53]\n",
            " [505]\n",
            " [  8]\n",
            " [260]\n",
            " [ 12]\n",
            " [390]\n",
            " [637]\n",
            " [  6]\n",
            " [ 12]\n",
            " [348]\n",
            " [ 34]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_FEATURES, EMBEDDING_DIM, input_length=4))\n",
        "model.add(Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4, activation='sigmoid'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='categorical_crossentropy',\n",
        "    # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    #loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJSUt5Hlra47",
        "outputId": "fc5f44f2-5eb5-43cc-e815-ded1c896f4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 4, 300)            1500000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 4, 256)           439296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " seq_self_attention_1 (SeqSe  (None, 4, 256)           16449     \n",
            " lfAttention)                                                    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,959,845\n",
            "Trainable params: 1,959,845\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train,\n",
        "          batch_size = BATCH_SIZE,\n",
        "          epochs = N_EPOCHS,\n",
        "          validation_data=(X_val,Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty-0qTDdnEvr",
        "outputId": "72f6f67b-8986-4d56-a8e4-8d6b6581cff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "352/352 [==============================] - 55s 116ms/step - loss: 0.9494 - accuracy: 0.7201 - val_loss: 0.7884 - val_accuracy: 0.7291\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 40s 113ms/step - loss: 0.7650 - accuracy: 0.7316 - val_loss: 0.7659 - val_accuracy: 0.7291\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 41s 117ms/step - loss: 0.7256 - accuracy: 0.7314 - val_loss: 0.7332 - val_accuracy: 0.7285\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 40s 115ms/step - loss: 0.7005 - accuracy: 0.7309 - val_loss: 0.7252 - val_accuracy: 0.7263\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 40s 112ms/step - loss: 0.6896 - accuracy: 0.7311 - val_loss: 0.7222 - val_accuracy: 0.7259\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 40s 114ms/step - loss: 0.6834 - accuracy: 0.7310 - val_loss: 0.7212 - val_accuracy: 0.7249\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 40s 115ms/step - loss: 0.6798 - accuracy: 0.7303 - val_loss: 0.7216 - val_accuracy: 0.7249\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 39s 112ms/step - loss: 0.6761 - accuracy: 0.7318 - val_loss: 0.7215 - val_accuracy: 0.7249\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 39s 112ms/step - loss: 0.6729 - accuracy: 0.7312 - val_loss: 0.7216 - val_accuracy: 0.7249\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 39s 111ms/step - loss: 0.6698 - accuracy: 0.7311 - val_loss: 0.7205 - val_accuracy: 0.7243\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f596b50ca10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, Y_test,\n",
        "               batch_size = BATCH_SIZE)\n",
        "\n",
        "predicted = [LABELS[np.argmax(i)] for i in model.predict(X_test)]\n",
        "actual = [LABELS[np.argmax(i)] for i in Y_test]\n",
        "np.savetxt(\"answer.csv\", predicted, delimiter=\",\", fmt='%s')\n",
        "report_score(actual,predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfz3nt_mQsE_",
        "outputId": "bc300dbc-120f-4468-9bb5-0ed6b3701e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199/199 [==============================] - 4s 18ms/step - loss: 0.9005 - accuracy: 0.7146\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |     7     |     0     |     6     |   1890    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |     0     |     0     |     1     |    696    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |    12     |     9     |    44     |   4399    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |    70     |    20     |    151    |   18108   |\n",
            "-------------------------------------------------------------\n",
            "Score: 4585.0 out of 11651.25\t(39.35200085827701%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39.35200085827701"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df.to_csv('output.csv', index=False, encoding = 'utf-8') \n",
        "files.download('answer.csv')"
      ],
      "metadata": {
        "id": "PASME56cVlyi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}